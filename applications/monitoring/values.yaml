# Configuration of Influx endpoint to receive monitoring data.
config:
  influxdbHostname: "monitoring.lsst.codes"
  influxdbOrg: "square"

  # -- Use prometheus config to specify all Prometheus endpoints on services
  # Except monitoring.  Monitoring is enabled if you're installing this chart,
  # but only if the influxdb2 component is installed, which is a global
  # singleton, would you have any Prometheus endpoint to scrape.
  prometheus:
    argocd:
      application_controller: "http://argocd-application-controller-metrics.argocd.svc:8082/metrics"
      notifications_controller: "http://argocd-notifications-controller-metrics.argocd.svc:9001/metrics"
      # redis: "http://argocd-redis-metrics.argocd.svc:9121/metrics"
      repo_server: "http://argocd-repo-server-metrics.argocd.svc:8084/metrics"
      server: "http://argocd-server-metrics.argocd.svc:8083/metrics"
    nublado:
      hub: "http://hub.nublado:8081/metrics"
    ingress-nginx:
      controller: "http://ingress-nginx-controller-metrics.ingress-nginx:10254/metrics"

# InfluxDB v2 server component.  Soon to be replaced with Influx DB v3.
influxdb2:
  # -- enable influxdb2 server at all?
  # @default -- False
  enabled: false

  # -- InfluxDB2 admin user; uses admin-password/admin-token keys from
  # secret.
  adminUser:
    # -- InfluxDB internal organization
    organization: "square"

    # -- Bucket to dump raw monitoring data into
    bucket: "monitoring"

    # -- User name
    user: "admin"

    # -- How long to keep data
    retention_policy: "30d"

    # -- Where we store secrets to run the server
    existingSecret: monitoring

  # -- InfluxDB2 ingress configuration.
  ingress:
    # @default -- False
    # This is a lie.  We define our own ingress template that knows how
    # to route to both influxdb2 and chronograf, and use it.
    enabled: false

  # -- InfluxDB2 startup probe.  We set the failure threshold high
  # because when influx has many full shards, it takes a very long time
  # to start up and check its shards, and that will cause a crash loop.
  # @default -- See `values.yaml`
  startupProbe:
    # -- Whether to enable a startup probe
    enabled: true

    # -- How long to wait before checking the first time
    initialDelaySeconds: 30

    # -- Period between checking whether InfluxDB has started
    periodSeconds: 10

    # -- Number of checks to conclude whether InfluxDB won't start.  High
    # to allow up to 10 minutes for startup; see above
    failureThreshold: 60

  # -- InfluxDB2 liveness probe.
  livenessProbe:
    # -- Period between checks for whether InfluxDB is still alive
    periodSeconds: 10

    # -- Number of checks to conclude whether InfluxDB has died
    failureThreshold: 10

  # -- Resource limits and requests for the InfluxDB server instance
  # @default -- See `values.yaml`
  resources:
    limits:
      cpu: 4.0
      memory: 30Gi
    requests:
      cpu: 1.0
      memory: 1Gi

# Chronograf provides a UI for analyzing the data in Influx.  Unlike the
# native InfluxDBv2 visualizer, it is capable of doing OIDC authentication.
chronograf:
  # -- enable chronograf at all?
  # @default -- False
  enabled: false

  ## Image Settings
  ##
  # -- chronograf image settings
  image:
    repository: "quay.io/influxdb/chronograf"
    tag: 1.10.3
    pullPolicy: IfNotPresent

  ## Specify a service type
  ## ClusterIP is default
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  # -- Chronograf service
  service:
    replicas: 1
    type: ClusterIP

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  # -- Chronograf resource requests/limits
  resources:
    requests:
      memory: 1024Mi
      cpu: 1
    limits:
      memory: 30Gi
      cpu: 4

  # -- Chronograf ingress
  ingress:
    enabled: false
    tls: false
    hostname: ""
    className: "nginx"
    path: /chronograf(/|$)

  ## Enable OAuth
  # -- Enable Chronograf oauth
  # Never enable it: it breaks the deployment because it's expecting a static
  # TOKEN_SECRET.  Instead, to get oauth working, leave this setting as-is
  # and just configure all the correct environment variables (see below).
  oauth:
    enabled: false

  # -- Environment for chronograf
  env:
    BASE_PATH: /chronograf
    CUSTOM_AUTO_REFRESH: "1s=1000"
    HOST_PAGE_DISABLED: true
    INFLUXDB_URL: "https://monitoring.lsst.codes"  # Expect this to change
    INFLUXDB_ORG: "square"

  # -- Chronograf will read environment variables from this secret
  # It expects keys generic_client_id, generic_client_secret,
  # and token_secret (assuming you want generic OIDC auth talking to your
  # local Gafaelfawr, which is usually the case).  You should also have
  # INFLUXDB_TOKEN for InfluxDBv2.  Check the Chronograf docs if you are
  # using some other setup (e.g. InfluxDBv1 for a database, or GitHub for
  # an auth provider.)
  envFromSecret: monitoring

  # -- Chronograf update strategy
  updateStrategy:
    type: Recreate

# Cronjobs to handle bucket and task creation for alert generation.
cronjob:
  # -- enable cronjobs at all?
  # You only need this once per influxdb instance.  It probably should
  # run in the same environment as influxdb, but that's not necessary.
  # @default -- False
  enabled: false

  # -- image for monitoring-related cronjobs
  image:
    # -- repository for rubin-influx-tools, which supplies tools and dashboards
    repository: ghcr.io/lsst-sqre/rubin-influx-tools
    # -- tag for rubin-influx-tools
    # @default -- the appVersion of the chart
    tag: ""
    # -- imagePullPolicy for cronjobs
    pullPolicy: "IfNotPresent"

  # -- set to true to enable debug logging
  debug: false

  # -- schedules for jobs
  schedule:
    # -- bucketmaker schedule
    bucketmaker: "*/15 * * * *"
    # -- bucketmapper schedule
    bucketmapper: "3-59/15 * * * *"
    # -- taskmaker schedule
    taskmaker: "6-59/15 * * * *"

# -- Gafaelfawr ingress for InfluxDBv2, only used if the service is enabled.
ingress:
  enabled: false
  influxdb2:
    # -- Additional annotations to add to the ingress
    annotations: {}

# Telegraf for Prometheus monitoring.
telegraf:
  # -- enable telegraf at all?
  enabled: true

  # -- config for telegraf
  # Use generated config instead of specifying anything here.
  config:
    processors: []
    inputs: []
    outputs: []

  # -- pod resources for telegraf
  resources:
    limits:
      memory: 512Mi
      cpu: 900m

  # -- arguments for telegraf invocation
  args:
  - "--config"
  - "/etc/telegraf-generated/telegraf-generated.conf"

  # -- rbac settings; we need the additional rules for Prometheus scraping.
  rbac:
    clusterWide: true

  # -- environment for telegraf
  env:
  - name: INFLUX_TOKEN
    valueFrom:
      secretKeyRef:
        key: telegraf-token
        name: monitoring

  # -- telegraf pod labels
  # This annotation is required in an RSP so telegraf can get metrics from
  # JupyterHub
  podLabels:
    hub.jupyter.org/network-access-hub: 'true'

  # -- use telegraf service
  service:
    enabled: false

  # -- template version
  tplVersion: 2

  # -- telegraf volume: generated configuration
  volumes:
  - name: telegraf-generated-config
    configMap:
      name: telegraf-generated-config

  # -- telegraf volume mount: generated configuration
  mountPoints:
  - name: telegraf-generated-config
    mountPath: /etc/telegraf-generated

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- services enabled in this RSP instance
  # @default -- Set by Argo CD
  enabledServices: ""

  # -- Host name for instance identification
  # @default -- Set by Argo CD
  host: ""

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: ""
