# -- Configuration of Influx endpoint to receive monitoring data
config:
  influxdbHostname: "monitoring.lsst.codes"
  influxdbOrg: "square"

  # -- Use prometheus config to specify all the services that expose
  # prometheus endpoints.
  prometheus:
    argocd:
      application_controller: "http://argocd-application-controller-metrics.argocd.svc:8082/metrics"
      notifications_controller: "http://argocd-notifications-controller-metrics.argocd.svc:9001/metrics"
      # redis: "http://argocd-redis-metrics.argocd.svc:9121/metrics"
      repo_server: "http://argocd-repo-server-metrics.argocd.svc:8084/metrics"
      server: "http://argocd-server-metrics.argocd.svc:8083/metrics"
    nublado:
      hub: "http://hub.nublado:8081/metrics"
    ingress-nginx:
      controller: "http://ingress-nginx-controller-metrics.ingress-nginx:10254/metrics"

# -- InfluxDB v2 server component.  Soon to be replaced with Influx DB v3
influxdb2:
  # -- enable influxdb2 server at all?
  # @default -- False
  enabled: false

  # -- InfluxDB2 admin user; uses admin-password/admin-token keys from
  # secret.
  adminUser:
    # -- InfluxDB internal organization
    organization: "square"

    # -- Bucket to dump raw monitoring data into
    bucket: "monitoring"

    # -- User name
    user: "admin"

    # -- How long to keep data
    retention_policy: "30d"

    # -- Where we store secrets to run the server
    existingSecret: monitoring

  # -- InfluxDB2 ingress configuration.
  ingress:
    # @default -- False
    # This is a lie.  We define our own ingress template that knows how
    # to route to both influxdb2 and chronograf, and use it.
    enabled: false

  # -- InfluxDB2 startup probe.  We set the failure threshold high
  # because when influx has many full shards, it takes a very long time
  # to start up and check its shards, and that will cause a crash loop.
  # @default -- See `values.yaml`
  startupProbe:
    # -- Whether to enable a startup probe
    enabled: true

    # -- How long to wait before checking the first time
    initialDelaySeconds: 30

    # -- Period between checking whether InfluxDB has started
    periodSeconds: 10

    # -- Number of checks to conclude whether InfluxDB won't start.  High
    # to allow up to 10 minutes for startup; see above
    failureThreshold: 60

  # -- InfluxDB2 liveness probe.
  livenessProbe:
    # -- Period between checks for whether InfluxDB is still alive
    periodSeconds: 10

    # -- Number of checks to conclude whether InfluxDB has died
    failureThreshold: 10

  # -- Resource limits and requests for the InfluxDB server instance
  # @default -- See `values.yaml`
  resources:
    limits:
      cpu: 4.0
      memory: 30Gi
    requests:
      cpu: 1.0
      memory: 1Gi

chronograf:
  # -- enable chronograf at all?
  # @default -- False
  enabled: false

  ## Image Settings
  ##
  image:
    repository: "quay.io/influxdb/chronograf"
    tag: 1.10.3
    pullPolicy: IfNotPresent

  ## Specify a service type
  ## ClusterIP is default
  ## ref: http://kubernetes.io/docs/user-guide/services/
  ##
  service:
    replicas: 1
    type: ClusterIP

  ## Configure resource requests and limits
  ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
  ##
  resources:
    requests:
      memory: 1024Mi
      cpu: 1
    limits:
      memory: 30Gi
      cpu: 4

  ingress:
    enabled: false
    tls: false
    hostname: ""
    className: "nginx"
    path: /chronograf(/|$)


  ## Enable OAuth
  oauth:
    enabled: false

  env:
    BASE_PATH: /chronograf
    CUSTOM_AUTO_REFRESH: "1s=1000"
    HOST_PAGE_DISABLED: true
    INFLUXDB_URL: "https://monitoring.lsst.codes"  # Expect this to change
    INFLUXDB_ORG: "square"
  ## INFLUXDB_TOKEN should be in the monitoring secret as well as
  ## TOKEN_SECRET and GH_CLIENT_SECRET.  Note that INFLUX_TOKEN is for
  ## InfluxDBv1 and INFLUXDB_TOKEN is for v2.
  # -- Chronograf expects keys generic_client_id, generic_client_secret,
  # and token_secret.

  envFromSecret: monitoring

  updateStrategy:
    type: Recreate

cronjob:
  # -- enable cronjobs at all?
  # You only need this once per influxdb instance.  It probably should
  # run in the same environment as influxdb, but that's not necessary.
  # @default -- False
  enabled: false

  # -- image for monitoring-related cronjobs
  image:
    # -- repository for rubin-influx-tools
    repository: ghcr.io/lsst-sqre/rubin-influx-tools
    # -- tag for rubin-influx-tools
    # @default -- the appVersion of the chart
    tag: ""
    # -- imagePullPolicy for cronjobs
    # @default -- "IfNotPresent"
    pullPolicy: "IfNotPresent"

  # -- set to true to enable debug logging
  debug: false

  # -- schedules for jobs
  schedule:
    # -- bucketmaker schedule
    bucketmaker: "*/15 * * * *"
    # -- bucketmapper schedule
    bucketmapper: "3-59/15 * * * *"
    # -- taskmaker schedule
    taskmaker: "6-59/15 * * * *"

# -- ingress for InfluxDBv2 Only used if the service is enabled.
ingress:

  chronograf:
    hostname: ""
    annotations: {}
  influxdb2:
    # -- Additional annotations to add to the ingress
    annotations: {}

# -- telegraf for Prometheus monitoring.
telegraf:
  # -- enable telegraf at all?
  # @default -- True

  enabled: true

  # Remove processors, inputs and outputs: use generated config instead.
  config:
    processors: []
    inputs: []
    outputs: []

  resources:
    limits:
      memory: 512Mi
      cpu: 900m

  args:
  - "--config"
  - "/etc/telegraf-generated/telegraf-generated.conf"

  # We need the additional rules for prometheus scraping.
  rbac:
    clusterWide: true

  env:
  - name: INFLUX_TOKEN
    valueFrom:
      secretKeyRef:
        key: telegraf-token
        name: monitoring

  podLabels:
    hub.jupyter.org/network-access-hub: 'true'

  service:
    enabled: false

  tplVersion: 2

  volumes:
  - name: telegraf-generated-config
    configMap:
      name: telegraf-generated-config

  mountPoints:
  - name: telegraf-generated-config
    mountPath: /etc/telegraf-generated

# The following will be set by parameters injected by Argo CD and should not
# be set in the individual environment values files.
global:
  # -- services enabled in this RSP instance
  # @default -- Set by Argo CD
  enabledServices: ""

  # -- Host name for instance identification
  # @default -- Set by Argo CD
  host: ""

  # -- Base path for Vault secrets
  # @default -- Set by Argo CD
  vaultSecretsPath: ""
